{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\">Facial Recognition for Seamless Student Identification</h1>\n",
    "<h3 style=\"text-align: center;\">Ghaisan Rabbani<br>Melin Ayu Safitri<br>Rachmawati Hapsari Putri</h3>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import cv2 # for computer vision tasks\n",
    "import pickle # for saving and loading data\n",
    "import numpy as np # for numerical operations and array handling\n",
    "import os # for interacting with the operating system\n",
    "import csv \n",
    "import time # for time-related functions\n",
    "import pyttsx3 # for text-to-speech (TTS) functionality\n",
    "from datetime import datetime # for manipulating date and time\n",
    "from deepface import DeepFace # for facial recognition and emotion analysis\n",
    "from threading import Thread # for concurrent execution of tasks\n",
    "# Others\n",
    "import warnings\n",
    "import pickle\n",
    "\n",
    "# Configure Settings\n",
    "warnings.simplefilter(\"ignore\")  # Ignore warnings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.12.5 | packaged by Anaconda, Inc. | (main, Sep 12 2024, 18:18:29) [MSC v.1929 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(f\"Python version: {sys.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inisialisasi text-to-speech\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "# Variable for tracking\n",
    "last_emotion_check = 0\n",
    "emotion_check_interval = 2.0\n",
    "current_emotion = \"Unknown\"\n",
    "current_emotion_accuracy = 0\n",
    "face_detection_confidence = 0\n",
    "last_speech_time = 0\n",
    "speech_interval = 5.0\n",
    "is_speaking = False\n",
    "\n",
    "# Column definitions for attendance CSV file\n",
    "COL_NAMES = ['NAME', 'TIME', 'EMOTION', 'EMOTION_ACCURACY', 'FACE_CONFIDENCE']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code Explanation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Initializes the `pyttsx3` library creates an instance of the TTS engine, which can be used later to convert text to speech. pyttsx3 works offline and supports various platforms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **last_emotion_check:** To avoid unnecessary checks and manage the frequency of emotion analysis (by comparing with emotion_check_interval).\n",
    "- **emotion_check_interval:** Controls how often the system checks for emotions from the face. For example, if this is set to 2.0, the program will check emotions every 2 seconds.\n",
    "- **current_emotion:** Keeps track of the most recent detected emotion, such as \"happy\", \"sad\", or \"angry\", which can be used later for feedback or analysis.\n",
    "- **current_emotion_accuracy:** Represents how confident the model is in detecting the current emotion. This could be used to filter out low-confidence results or display feedback based on the certainty of the emotion.\n",
    "- **face_detection_confidence:** Helps ensure that face detection is reliable before performing tasks like emotion analysis or speech output. Can be used to filter out false detections.\n",
    "- **last_speech_time:** Helps control how often the system speaks by checking if the time interval (speech_interval) has passed since the last speech output.\n",
    "- **speech_interval:** Prevents the system from speaking too frequently. For example, if the system says something every 5 seconds, speech_interval is set to 5.0.\n",
    "- **is_speaking:** Prevents overlapping speech. This ensures that only one speech instance can occur at a time, allowing the system to wait for speech to finish before starting another one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def speak_async(text):\n",
    "    global is_speaking\n",
    "    if not is_speaking:\n",
    "        is_speaking = True\n",
    "        engine.say(text)\n",
    "        engine.runAndWait()\n",
    "        is_speaking = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code Explanation:**\n",
    "\n",
    "- The `speak_async function` performs text-to-speech operations asynchronously by:\n",
    "    - Checking if the system is already speaking (using the `is_speaking` flag).\n",
    "    - If not speaking, it starts the speech process and waits for it to complete.\n",
    "    - Once done, it sets the is_speaking flag to False, allowing further speech commands to be processed.\n",
    "- Use Case:\n",
    "    This function ensures that the TTS engine doesn't overlap with itself and avoids issues where multiple speech requests might conflict or overlap. It's helpful in scenarios where you might have multiple speech outputs in a program (such as providing feedback or notifications)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_attendance(name, emotion, emotion_acc, face_conf):\n",
    "    try:\n",
    "        ts = time.time()\n",
    "        date = datetime.fromtimestamp(ts).strftime(\"%d-%m-%Y\")\n",
    "        timestamp = datetime.fromtimestamp(ts).strftime(\"%H:%M:%S\")\n",
    "        \n",
    "        if not os.path.exists(\"Attendance\"):\n",
    "            os.makedirs(\"Attendance\")\n",
    "            \n",
    "        attendance_file = f\"Attendance/Attendance_{date}.csv\"\n",
    "        file_exists = os.path.isfile(attendance_file)\n",
    "        \n",
    "        # Add accuracy to attendance data\n",
    "        attendance_data = [\n",
    "            str(name), \n",
    "            str(timestamp), \n",
    "            str(emotion),\n",
    "            f\"{emotion_acc:.2f}%\",\n",
    "            f\"{face_conf:.2f}%\"\n",
    "        ]\n",
    "        \n",
    "        with open(attendance_file, \"a\", newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            if not file_exists:\n",
    "                writer.writerow(COL_NAMES)\n",
    "            writer.writerow(attendance_data)\n",
    "            \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error recording attendance: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code Explanation:**\n",
    "\n",
    "- **Function:** record_attendance(`name`, `emotion`, `emotion_acc`, `face_conf`)\n",
    "- **Purpose:** This function records a person's attendance along with additional data like their detected emotion, the accuracy of the emotion detection, and the confidence of the face detection. It logs this data into a CSV file under a folder named `Attendance`, creating the file if it doesn't exist.\n",
    "- **Uses:**\n",
    "    - It ensures proper organization by creating an \"Attendance\" folder.\n",
    "    - It allows adding new records of attendance throughout the day, appending data to the file for each person detected.\n",
    "    - The function tracks both time and emotion for each attendance record and provides feedback on the accuracy of the emotion and face detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_emotion_async(frame, name):\n",
    "    global current_emotion, last_speech_time\n",
    "    try:\n",
    "        analysis = DeepFace.analyze(frame, actions=['emotion'], enforce_detection=False)\n",
    "        new_emotion = analysis[0]['dominant_emotion']\n",
    "        \n",
    "        # If emotions change and have passed the speech interval\n",
    "        current_time = time.time()\n",
    "        if (new_emotion != current_emotion and \n",
    "            current_time - last_speech_time > speech_interval):\n",
    "            \n",
    "            current_emotion = new_emotion\n",
    "            # Create messages based on emotions\n",
    "            if current_emotion == 'happy':\n",
    "                message = f\"Hi {name}, your mood today is happy! Keep up the positive energy!\"\n",
    "            elif current_emotion == 'sad':\n",
    "                message = f\"Hi {name}, your mood today is sad. Stay strong and positive!\"\n",
    "            elif current_emotion == 'angry':\n",
    "                message = f\"Hi {name}, your mood today is angry. Take a deep breath and relax.\"\n",
    "            else:\n",
    "                message = f\"Hi {name}, your mood is {current_emotion}. Stay positive!\"\n",
    "            \n",
    "            # Run speech in a separate thread\n",
    "            Thread(target=speak_async, args=(message,)).start()\n",
    "            last_speech_time = current_time\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(\"Error analyzing emotion:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `analyze_emotion_async function` is a crucial part of an emotion-based feedback system.<br>\n",
    "**It:**\n",
    "- Analyzes emotions from a video frame using the DeepFace library.\n",
    "- Provides verbal feedback based on detected emotions.\n",
    "- Ensures the system operates efficiently by managing timing intervals for speech.\n",
    "- Runs asynchronously, keeping the main program responsive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KNeighborsClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;KNeighborsClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\">?<span>Documentation for KNeighborsClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>KNeighborsClassifier()</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "KNeighborsClassifier()"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Starting video capture\n",
    "video = cv2.VideoCapture(0)\n",
    "\n",
    "# Load classifier and data\n",
    "facedetect = cv2.CascadeClassifier(r'data\\haarcascade_frontalface_default.xml')\n",
    "with open(r'data\\names.pkl', 'rb') as f:\n",
    "    LABELS = pickle.load(f)\n",
    "with open(r'data\\faces_data.pkl', 'rb') as f:\n",
    "    FACES = pickle.load(f)\n",
    "\n",
    "# Setup KNN\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(FACES, LABELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code Explanation:**\n",
    "- `cv2.VideoCapture(0)`: This is the starting point for capturing live frames from the webcam for processing, such as face detection and recognition.\n",
    "- `haarcascade_frontalface_default.xml` file contains the trained model for detecting frontal faces.\n",
    "- `cv2.CascadeClassifier()` initializes the classifier using the provided XML file.\n",
    "- `Usage`: This classifier will be used later to detect the location of faces in the video frames.\n",
    "- The code is part of a real-time face recognition system: Match detected faces with the pre-trained face data using the `KNN classifier`.\n",
    "- Applications:\n",
    "    - Attendance systems.\n",
    "    - Access control systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How It Works Together**\n",
    "- `Video Capture:` Starts the webcam feed for real-time video processing.\n",
    "- `Face Detection:` Uses the Haar Cascade classifier to locate faces in the video frames.\n",
    "- `Data Loading:` Loads pre-trained face data and corresponding labels for recognition.\n",
    "- `KNN Training:` Trains a KNN classifier to recognize faces based on the pre-loaded data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable for tracking attendance\n",
    "last_attendance_name = None\n",
    "last_attendance_time = 0\n",
    "attendance_cooldown = 30  # 30 second cooldown between attendance for the same name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Tracking Attendance:**<br>\n",
    "    - `last_attendance_name` ensures that only unique attendance events are logged in quick succession.\n",
    "    - If the recognized name matches `last_attendance_name`, the system checks the cooldown before allowing another entry.\n",
    "- **Cooldown Mechanism:**<br>\n",
    "    - `last_attendance_time` is updated every time a person's attendance is recorded.\n",
    "    - If the current time minus `last_attendance_time` is less than attendance_cooldown, the system skips recording attendance for that person.\n",
    "- **Prevention of Duplicates:**<br>\n",
    "    - The combination of these variables avoids creating multiple entries for the same person within a short time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    ret, frame = video.read()\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Face detection with confidence\n",
    "    faces = facedetect.detectMultiScale(\n",
    "        gray,\n",
    "        scaleFactor=1.3,\n",
    "        minNeighbors=5,\n",
    "        minSize=(30, 30),\n",
    "        flags=cv2.CASCADE_SCALE_IMAGE\n",
    "    )\n",
    "    \n",
    "    # Calculate confidence score for face detection\n",
    "    if len(faces) > 0:\n",
    "        # Face size is used for detection as a confidence indicator.\n",
    "        x, y, w, h = faces[0]\n",
    "        face_area = w * h\n",
    "        frame_area = frame.shape[0] * frame.shape[1]\n",
    "        face_detection_confidence = min((face_area / frame_area) * 400, 100)  # Scale dan cap at 100%\n",
    "    \n",
    "    for (x,y,w,h) in faces:\n",
    "        crop_img = frame[y:y+h, x:x+w, :]\n",
    "        resized_img = cv2.resize(crop_img, (50,50)).flatten().reshape(1,-1)\n",
    "        output = knn.predict(resized_img)\n",
    "        current_name = str(output[0])\n",
    "        \n",
    "        current_time = time.time()\n",
    "        if current_time - last_emotion_check > emotion_check_interval:\n",
    "            Thread(target=analyze_emotion_async, args=(frame.copy(), current_name)).start()\n",
    "            last_emotion_check = current_time\n",
    "        \n",
    "        # Draw rectangles and information\n",
    "        cv2.rectangle(frame, (x,y), (x+w, y+h), (0,0,255), 1)\n",
    "        cv2.rectangle(frame,(x,y-40),(x+w,y),(50,50,255),-1)\n",
    "        cv2.putText(frame, current_name, (x,y-15), cv2.FONT_HERSHEY_COMPLEX, 1, (255,255,255), 1)\n",
    "        \n",
    "        # Display emotion and accuracies\n",
    "        y_offset = y + h + 25\n",
    "        emotion_text = f\"Emotion: {current_emotion} ({current_emotion_accuracy:.1f}%)\"\n",
    "        cv2.putText(frame, emotion_text, (x, y_offset), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,255,0), 2)\n",
    "        \n",
    "        face_conf_text = f\"Face Detection Confidence: {face_detection_confidence:.1f}%\"\n",
    "        cv2.putText(frame, face_conf_text, (x, y_offset + 25), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,255,0), 2)\n",
    "        \n",
    "        # Add instruction text\n",
    "        cv2.putText(frame, \"Press 'o' to mark attendance\", (10, 30), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    \n",
    "    k = cv2.waitKey(1)\n",
    "    \n",
    "    # Handle attendance recording\n",
    "    if k == ord('o') and len(faces) > 0:\n",
    "        current_time = time.time()\n",
    "        if (last_attendance_name != current_name or \n",
    "            current_time - last_attendance_time > attendance_cooldown):\n",
    "            \n",
    "            Thread(target=speak_async, args=(\"Recording attendance...\",)).start()\n",
    "            \n",
    "            if record_attendance(current_name, current_emotion, \n",
    "                               current_emotion_accuracy, face_detection_confidence):\n",
    "                last_attendance_name = current_name\n",
    "                last_attendance_time = current_time\n",
    "                Thread(target=speak_async, args=(\"Attendance recorded successfully!\",)).start()\n",
    "            else:\n",
    "                Thread(target=speak_async, args=(\"Failed to record attendance.\",)).start()\n",
    "        break\n",
    "    \n",
    "    if k == ord('q'):\n",
    "        break\n",
    "\n",
    "video.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code integrates multiple functionalities into a cohesive real-time system:<br>\n",
    "\n",
    "- Detecting faces and calculating detection confidence.\n",
    "- Recognizing faces using a pre-trained KNN model.\n",
    "- Analyzing emotions with periodic checks.\n",
    "- Recording attendance with checks to prevent duplicate entries.\n",
    "- Providing a user-friendly interface with visual information and text-to-speech notifications.\n",
    "- The program continuously processes video frames until the user stops it or attendance is recorded."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jcds0412",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
